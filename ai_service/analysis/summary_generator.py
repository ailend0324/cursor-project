import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
import re
import jieba
import jieba.analyse
from collections import Counter, defaultdict

# È°πÁõÆÊ†πÁõÆÂΩï
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

def load_data(excel_path):
    """Âä†ËΩΩExcelÊï∞ÊçÆÊñá‰ª∂"""
    try:
        print(f"Ê≠£Âú®Âä†ËΩΩÊï∞ÊçÆ: {excel_path}")
        df = pd.read_excel(excel_path)
        print(f"ÊàêÂäüÂä†ËΩΩ {len(df)} Êù°ËÆ∞ÂΩï")
        return df
    except Exception as e:
        print(f"Âä†ËΩΩÊï∞ÊçÆÂ§±Ë¥•: {e}")
        return None

def load_faq_data():
    """Âä†ËΩΩFAQÊï∞ÊçÆ"""
    faq_path = os.path.join(PROJECT_ROOT, 'knowledge_base', 'faq.json')
    try:
        with open(faq_path, 'r', encoding='utf-8') as f:
            faq_data = json.load(f)
        print(f"ÊàêÂäüÂä†ËΩΩ {len(faq_data)} ‰∏™FAQÊù°ÁõÆ")
        return faq_data
    except Exception as e:
        print(f"Âä†ËΩΩFAQÊï∞ÊçÆÂ§±Ë¥•: {e}")
        return []

def preprocess_text(text):
    """È¢ÑÂ§ÑÁêÜÊñáÊú¨"""
    if not isinstance(text, str):
        return ""
    
    # ÁßªÈô§URL
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    # ÁßªÈô§ÁâπÊÆäÂ≠óÁ¨¶
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
    
    # ÁßªÈô§Â§ö‰ΩôÁöÑÁ©∫Ê†º
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_keywords(text, top_n=5):
    """ÊèêÂèñÊñáÊú¨‰∏≠ÁöÑÂÖ≥ÈîÆËØç"""
    text = preprocess_text(text)
    if not text:
        return []
    
    # ‰ΩøÁî®jiebaÊèêÂèñÂÖ≥ÈîÆËØç
    keywords = jieba.analyse.extract_tags(text, topK=top_n)
    return keywords

def group_messages_by_conversation(df):
    """ÊåâÂØπËØùIDÂàÜÁªÑÊ∂àÊÅØÂπ∂ËÆ°ÁÆóÊó∂Èïø"""
    conversations = []
    
    for touch_id, group in df.groupby('touch_id'):
        # ÊåâÊó∂Èó¥ÊéíÂ∫è - ‰ΩøÁî®user_start_timeËÄå‰∏çÊòØtimestamp
        if 'user_start_time' in group.columns:
            group = group.sort_values('user_start_time')
        
        # ËÆ°ÁÆóÂØπËØùÊó∂Èïø
        if 'user_start_time' in group.columns and 'user_end_time' in group.columns:
            start_time = group['user_start_time'].min()
            end_time = group['user_end_time'].max()
            
            if pd.notna(start_time) and pd.notna(end_time):
                duration_minutes = (end_time - start_time).total_seconds() / 60
            else:
                duration_minutes = None
        else:
            duration_minutes = None
        
        # ÂàõÂª∫ÂØπËØùÂ≠óÂÖ∏
        conversation = {
            'touch_id': touch_id,
            'messages': group.to_dict('records'),
            'message_count': len(group),
            'start_time': start_time if 'start_time' in locals() else None,
            'end_time': end_time if 'end_time' in locals() else None,
            'duration_minutes': duration_minutes
        }
        
        conversations.append(conversation)
    
    print(f"Â∑≤ÂàÜÁªÑ {len(conversations)} ‰∏™ÂØπËØù")
    return conversations

def analyze_conversation_intent(messages, faq_data):
    """ÂàÜÊûêÂØπËØùÊÑèÂõæ"""
    # ÊèêÂèñÁî®Êà∑Ê∂àÊÅØ
    user_messages = [msg for msg in messages if msg.get('sender_type') == 1.0]
    
    if not user_messages:
        return {
            'primary_intent': 'Êú™Áü•',
            'matched_faqs': [],
            'confidence': 0
        }
    
    # Ëé∑ÂèñÂàùÂßãÁî®Êà∑Ê∂àÊÅØ
    initial_user_messages = user_messages[:min(3, len(user_messages))]
    initial_text = " ".join([str(msg.get('content', '')) for msg in initial_user_messages])
    
    # ÊèêÂèñÂÖ≥ÈîÆËØç
    keywords = extract_keywords(initial_text, top_n=8)
    
    # ÂåπÈÖçFAQ
    matched_faqs = []
    
    for faq in faq_data:
        # ËÆ°ÁÆóÂÖ≥ÈîÆËØçÂåπÈÖçÂ∫¶
        faq_keywords = faq.get('keywords', [])
        matched_keywords = [kw for kw in keywords if kw in faq.get('question', '') or kw in faq_keywords]
        
        if matched_keywords:
            similarity = len(matched_keywords) / len(keywords) if keywords else 0
            
            if similarity > 0.1:  # ËÆæÁΩÆÈòàÂÄº
                matched_faqs.append({
                    'id': faq.get('id'),
                    'category': faq.get('category', 'Êú™ÂàÜÁ±ª'),
                    'question': faq.get('question', ''),
                    'similarity': similarity,
                    'matched_keywords': matched_keywords
                })
    
    # ÊéíÂ∫èÂåπÈÖçÁªìÊûú
    matched_faqs = sorted(matched_faqs, key=lambda x: x['similarity'], reverse=True)
    
    # Á°ÆÂÆö‰∏ªË¶ÅÊÑèÂõæ
    primary_intent = matched_faqs[0]['category'] if matched_faqs else 'ÂÖ∂‰ªñ'
    confidence = matched_faqs[0]['similarity'] if matched_faqs else 0
    
    return {
        'primary_intent': primary_intent,
        'matched_faqs': matched_faqs[:3],  # Âè™ËøîÂõûÂâç3‰∏™ÂåπÈÖç
        'confidence': confidence
    }

def analyze_conversation_satisfaction(messages):
    """ËØÑ‰º∞Áî®Êà∑Êª°ÊÑèÂ∫¶"""
    # ÊèêÂèñÁî®Êà∑Ê∂àÊÅØ
    user_messages = [msg for msg in messages if msg.get('sender_type') == 1.0]
    
    if not user_messages:
        return {
            'satisfaction': 'Êú™Áü•',
            'confidence': 0,
            'indicators': []
        }
    
    # ÊúÄÂêéÂá†Êù°Áî®Êà∑Ê∂àÊÅØ
    last_user_messages = user_messages[-min(3, len(user_messages)):]
    last_text = " ".join([str(msg.get('content', '')) for msg in last_user_messages])
    
    # ÂÆö‰πâÊåáÊ†áËØçÊ±á
    satisfaction_indicators = {
        'Êª°ÊÑè': ['Ë∞¢Ë∞¢', 'ÊÑüË∞¢', 'Â•ΩÁöÑ', 'ÊòéÁôΩ‰∫Ü', 'Ëß£ÂÜ≥‰∫Ü', 'ÊáÇ‰∫Ü', 'ÈùûÂ∏∏Â•Ω', 'ÂæàÂ•Ω', 'Â§™Â•Ω‰∫Ü', 'üëç'],
        '‰∏çÊª°ÊÑè': ['‰∏çË°å', 'Ê≤°Áî®', 'Ê≤°Ëß£ÂÜ≥', '‰∏çÂØπ', 'ÈîôËØØ', 'ÁÉ¶‰∫∫', 'ËΩ¨‰∫∫Â∑•', '‰∫∫Â∑•', 'Â∑ÆËØÑ', '‰∏çÊòéÁôΩ', 'Âê¨‰∏çÊáÇ', 'ËΩ¨Êé•'],
        '‰∏≠ÊÄß': ['Âì¶', 'ÂóØ', 'Áü•ÈÅì‰∫Ü', 'ÂÜçËßÅ', 'ÊãúÊãú']
    }
    
    # ËÆ°Êï∞ÂåπÈÖçÁöÑÊåáÊ†á
    matched_indicators = {category: [] for category in satisfaction_indicators}
    
    for category, indicators in satisfaction_indicators.items():
        for indicator in indicators:
            if indicator in last_text:
                matched_indicators[category].append(indicator)
    
    # ËÆ°ÁÆóÊª°ÊÑèÂ∫¶ÂàÜÊï∞
    satisfaction_score = len(matched_indicators['Êª°ÊÑè']) - len(matched_indicators['‰∏çÊª°ÊÑè'])
    
    # Á°ÆÂÆöÊúÄÁªàÊª°ÊÑèÂ∫¶
    if satisfaction_score > 0:
        satisfaction = 'Êª°ÊÑè'
        confidence = min(1.0, satisfaction_score / 3)
    elif satisfaction_score < 0:
        satisfaction = '‰∏çÊª°ÊÑè'
        confidence = min(1.0, abs(satisfaction_score) / 3)
    else:
        # Â¶ÇÊûúÊúâ‰∏≠ÊÄßÊåáÊ†á‰ΩÜÊ≤°ÊúâÂÖ∂‰ªñÊåáÊ†á
        if matched_indicators['‰∏≠ÊÄß'] and not (matched_indicators['Êª°ÊÑè'] or matched_indicators['‰∏çÊª°ÊÑè']):
            satisfaction = '‰∏≠ÊÄß'
            confidence = min(1.0, len(matched_indicators['‰∏≠ÊÄß']) / 3)
        else:
            satisfaction = 'Êú™Áü•'
            confidence = 0
    
    # Êî∂ÈõÜÊâÄÊúâÂåπÈÖçÁöÑÊåáÊ†á
    all_indicators = []
    for category, indicators in matched_indicators.items():
        all_indicators.extend(indicators)
    
    return {
        'satisfaction': satisfaction,
        'confidence': confidence,
        'indicators': all_indicators
    }

def generate_conversation_summary(conversation):
    """ÁîüÊàêÂØπËØùÊëòË¶Å"""
    messages = conversation.get('messages', [])
    
    if not messages:
        return {
            'summary': 'Á©∫ÂØπËØù',
            'keywords': [],
            'details': {}
        }
    
    # ËÆ°ÁÆóÂü∫Êú¨ÁªüËÆ°Êï∞ÊçÆ
    user_messages = [msg for msg in messages if msg.get('sender_type') == 1.0]
    bot_messages = [msg for msg in messages if msg.get('sender_type') == 2.0]
    
    user_message_count = len(user_messages)
    bot_message_count = len(bot_messages)
    
    # ÊèêÂèñÁî®Êà∑Á¨¨‰∏ÄÊù°Ê∂àÊÅØ
    initial_question = user_messages[0].get('content', '') if user_messages else 'Êó†ÂàùÂßãÈóÆÈ¢ò'
    
    # ÊèêÂèñÂØπËØùÂÖ≥ÈîÆËØç
    all_user_content = " ".join([str(msg.get('content', '')) for msg in user_messages])
    keywords = extract_keywords(all_user_content, top_n=8)
    
    # Ëé∑ÂèñÊúÄÂêé‰∏ÄÊù°Êú∫Âô®‰∫∫ÂõûÂ§ç
    final_response = bot_messages[-1].get('content', '') if bot_messages else 'Êó†ÊúÄÁªàÂõûÂ§ç'
    
    # Ê£ÄÊü•ÊòØÂê¶Êúâ‰∫∫Â∑•ËΩ¨Êé•
    transfer_request = False
    transfer_keywords = ["ËΩ¨‰∫∫Â∑•", "‰∫∫Â∑•ÂÆ¢Êúç", "Áúü‰∫∫", "ËΩ¨Êé•"]
    
    for msg in user_messages:
        content = str(msg.get('content', ''))
        if any(keyword in content for keyword in transfer_keywords):
            transfer_request = True
            break
    
    # ÂàõÂª∫ÊëòË¶ÅÂØπË±°
    summary = {
        'touch_id': conversation.get('touch_id'),
        'start_time': conversation.get('start_time'),
        'end_time': conversation.get('end_time'),
        'duration_minutes': conversation.get('duration_minutes'),
        'message_count': conversation.get('message_count'),
        'user_message_count': user_message_count,
        'bot_message_count': bot_message_count,
        'initial_question': initial_question,
        'keywords': keywords,
        'final_response': final_response,
        'transfer_request': transfer_request
    }
    
    return summary

def analyze_common_paths(conversations, limit=100):
    """ÂàÜÊûêÂÖ±ÂêåÂØπËØùË∑ØÂæÑÂèäÂÖ∂È¢ëÁéá"""
    # ‰ªÖÂàÜÊûêÈôêÂÆöÊï∞ÈáèÁöÑÂØπËØù
    sample_conversations = conversations[:limit]
    
    # ÁªüËÆ°ÂàùÂßãÈóÆÈ¢ò
    initial_questions = []
    
    for conv in sample_conversations:
        messages = conv.get('messages', [])
        user_messages = [msg for msg in messages if msg.get('sender_type') == 1.0]
        
        if user_messages:
            initial_question = user_messages[0].get('content', '')
            keywords = extract_keywords(initial_question, top_n=3)
            initial_questions.append(" ".join(keywords))
    
    # ËÆ°ÁÆóÈ¢ëÁéá
    question_counter = Counter(initial_questions)
    
    # ËøîÂõûÊúÄÂ∏∏ËßÅÁöÑÂàùÂßãÈóÆÈ¢ò
    common_questions = question_counter.most_common(10)
    
    return common_questions

def generate_conversation_summaries(excel_path=None, output_path=None):
    """ÁîüÊàêÊâÄÊúâÂØπËØùÊëòË¶Å"""
    # ËÆæÁΩÆÈªòËÆ§Ë∑ØÂæÑ
    if excel_path is None:
        excel_path = os.path.join(PROJECT_ROOT, 'data', 'merged_chat_records.xlsx')
    
    if output_path is None:
        output_dir = os.path.join(PROJECT_ROOT, 'knowledge_base', 'summaries')
        os.makedirs(output_dir, exist_ok=True)
        
        # ÂàõÂª∫Â∏¶Êó∂Èó¥Êà≥ÁöÑÊñá‰ª∂Âêç
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = os.path.join(output_dir, f'conversation_summaries_{timestamp}.json')
    
    # Âä†ËΩΩÊï∞ÊçÆ
    df = load_data(excel_path)
    if df is None:
        return False
    
    # Âä†ËΩΩFAQÊï∞ÊçÆ
    faq_data = load_faq_data()
    
    # ÂàÜÁªÑÂØπËØù
    conversations = group_messages_by_conversation(df)
    
    # ÁîüÊàêÊëòË¶Å
    print("Ê≠£Âú®ÁîüÊàêÂØπËØùÊëòË¶Å...")
    summaries = []
    
    for i, conversation in enumerate(conversations):
        if i % 100 == 0:
            print(f"Â∑≤Â§ÑÁêÜ {i}/{len(conversations)} ‰∏™ÂØπËØù")
        
        # Âü∫Êú¨ÊëòË¶Å
        summary = generate_conversation_summary(conversation)
        
        # ÊÑèÂõæÂàÜÊûê
        intent_analysis = analyze_conversation_intent(conversation.get('messages', []), faq_data)
        summary['intent'] = intent_analysis
        
        # Êª°ÊÑèÂ∫¶ÂàÜÊûê
        satisfaction_analysis = analyze_conversation_satisfaction(conversation.get('messages', []))
        summary['satisfaction'] = satisfaction_analysis
        
        summaries.append(summary)
    
    # ÂàÜÊûêÂÖ±ÂêåË∑ØÂæÑ
    common_paths = analyze_common_paths(conversations)
    
    # ÂàõÂª∫ÂÆåÊï¥ÁªìÊûú
    result = {
        'summaries': summaries,
        'common_paths': common_paths,
        'metadata': {
            'total_conversations': len(conversations),
            'generated_at': datetime.now().isoformat()
        }
    }
    
    # ‰øùÂ≠òÁªìÊûú
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ÊëòË¶ÅÂ∑≤‰øùÂ≠òËá≥: {output_path}")
        return True
    except Exception as e:
        print(f"‰øùÂ≠òÊëòË¶ÅÂ§±Ë¥•: {e}")
        return False

if __name__ == "__main__":
    import sys
    
    excel_path = None
    output_path = None
    
    if len(sys.argv) > 1:
        excel_path = sys.argv[1]
    
    if len(sys.argv) > 2:
        output_path = sys.argv[2]
    
    generate_conversation_summaries(excel_path, output_path) 